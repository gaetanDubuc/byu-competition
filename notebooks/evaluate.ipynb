{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "fd418876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "import os\n",
    "\n",
    "\n",
    "data_path = \"../input\"\n",
    "test_dir = os.path.join(data_path, \"test\")\n",
    "submission_path = \"../working/submission.csv\"\n",
    "\n",
    "# Model path - adjust if your best model is saved in a different location\n",
    "model_path = \"/kaggle/input/train-yolo/yolo_weights/motor_detector/weights/best.pt\"\n",
    "\n",
    "CONCENTRATION = 0.1\n",
    "TRESHOLD = 0.10\n",
    "NMS_IOU_THRESHOLD = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "edbb649d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tomograms: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_classes mismatch: pretrain weights has 0 classes, but your model has 90 classes\n",
      "reinitializing detection head with 0 classes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrain weights\n",
      "Total slices in tomo_003acc: 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:03<00:00, 12.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total slices in tomo_00e047: 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 33.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total slices in tomo_01a877: 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 33.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Submission complete!\n",
      "Submission saved to: ../working/submission.csv\n",
      "\n",
      "Submission preview:\n",
      "       tomo_id  Motor axis 0  Motor axis 1  Motor axis 2\n",
      "0  tomo_003acc           183          1049           797\n",
      "1  tomo_00e047           175           539           600\n",
      "2  tomo_01a877           113           622           151\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from rfdetr import RFDETRBase\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision.transforms.functional as F\n",
    "import tqdm\n",
    "from itertools import chain\n",
    "\n",
    "\n",
    "def get_tomos(test_dir):\n",
    "    test_tomos = sorted(\n",
    "        [d for d in os.listdir(test_dir) if os.path.isdir(os.path.join(test_dir, d))]\n",
    "    )\n",
    "\n",
    "    total_tomos = len(test_tomos)\n",
    "    print(f\"Total tomograms: {total_tomos}\")\n",
    "    return test_tomos\n",
    "\n",
    "\n",
    "def get_slice_files_from_tomo_id(tomo_id, test_dir, concentration):\n",
    "    tomo_dir = os.path.join(test_dir, tomo_id)\n",
    "    slice_files = sorted([f for f in os.listdir(tomo_dir) if f.endswith(\".jpg\")])\n",
    "\n",
    "    # Apply CONCENTRATION to reduce the number of slices processed\n",
    "    # This will process approximately CONCENTRATION fraction of all slices\n",
    "    selected_indices = np.linspace(\n",
    "        0, len(slice_files) - 1, int(len(slice_files) * concentration)\n",
    "    )\n",
    "    selected_indices = np.round(selected_indices).astype(int)\n",
    "    slice_files = [slice_files[i] for i in selected_indices]\n",
    "    print(f\"Total slices in {tomo_id}: {len(slice_files)}\")\n",
    "\n",
    "    return slice_files\n",
    "\n",
    "\n",
    "def load_images(slice_files, tomo_dir):\n",
    "    images = {\n",
    "        os.path.join(tomo_dir, f): Image.open(os.path.join(tomo_dir, f)).convert(\"RGB\")\n",
    "        for f in slice_files\n",
    "    }\n",
    "    return images\n",
    "\n",
    "\n",
    "import supervision as sv\n",
    "\n",
    "\n",
    "def get_center_from_2dbox(box: np.ndarray):\n",
    "    x1, y1, x2, y2 = box\n",
    "\n",
    "    # Calculate center coordinates\n",
    "    x_center = (x1 + x2) / 2\n",
    "    y_center = (y1 + y2) / 2\n",
    "\n",
    "    return x_center, y_center\n",
    "\n",
    "\n",
    "def get_slice_number_from_filename(filename: str):\n",
    "    # Extract the slice number from the filename\n",
    "    # Assuming the filename format is \"slice_1.jpg\", \"slice_2.jpg\", etc.\n",
    "    # Adjust the parsing logic based on your actual filename format\n",
    "    slice_number = int(filename.split(\"/\")[-1].split(\".\")[0].split(\"_\")[-1])\n",
    "    return slice_number\n",
    "\n",
    "\n",
    "def make_center_predictions(detection: sv.Detections, slice_file):\n",
    "    all_detections = []\n",
    "    for idx, confidence in enumerate(detection.confidence):\n",
    "        slice_num = get_slice_number_from_filename(slice_file)\n",
    "        x_center, y_center = get_center_from_2dbox(detection.xyxy[idx])\n",
    "        all_detections.append(\n",
    "            {\n",
    "                \"z\": round(slice_num),\n",
    "                \"y\": round(y_center),\n",
    "                \"x\": round(x_center),\n",
    "                \"confidence\": float(confidence),\n",
    "            }\n",
    "        )\n",
    "    return all_detections\n",
    "\n",
    "\n",
    "def perform_3d_nms(detections, iou_threshold):\n",
    "    \"\"\"\n",
    "    Perform 3D Non-Maximum Suppression on detections to merge nearby motors\n",
    "    \"\"\"\n",
    "    if not detections:\n",
    "        return []\n",
    "\n",
    "    # Sort by confidence (highest first)\n",
    "    detections = sorted(detections, key=lambda x: x[\"confidence\"], reverse=True)\n",
    "\n",
    "    # List to store final detections after NMS\n",
    "    final_detections = []\n",
    "\n",
    "    # Define 3D distance function\n",
    "    def distance_3d(d1, d2):\n",
    "        return np.sqrt(\n",
    "            (d1[\"z\"] - d2[\"z\"]) ** 2\n",
    "            + (d1[\"y\"] - d2[\"y\"]) ** 2\n",
    "            + (d1[\"x\"] - d2[\"x\"]) ** 2\n",
    "        )\n",
    "\n",
    "    # Maximum distance threshold (based on box size and slice gap)\n",
    "    box_size = 24  # Same as annotation box size\n",
    "    distance_threshold = box_size * iou_threshold\n",
    "\n",
    "    # Process each detection\n",
    "    while detections:\n",
    "        # Take the detection with highest confidence\n",
    "        best_detection = detections.pop(0)\n",
    "        final_detections.append(best_detection)\n",
    "\n",
    "        # Filter out detections that are too close to the best detection\n",
    "        detections = [\n",
    "            d for d in detections if distance_3d(d, best_detection) > distance_threshold\n",
    "        ]\n",
    "\n",
    "    return final_detections\n",
    "\n",
    "\n",
    "def format_for_submission(tomo_id, final_detections):\n",
    "    # Sort detections by confidence (highest first)\n",
    "    final_detections.sort(key=lambda x: x[\"confidence\"], reverse=True)\n",
    "\n",
    "    # If there are no detections, return NA values\n",
    "    if not final_detections:\n",
    "        return {\n",
    "            \"tomo_id\": tomo_id,\n",
    "            \"Motor axis 0\": -1,\n",
    "            \"Motor axis 1\": -1,\n",
    "            \"Motor axis 2\": -1,\n",
    "        }\n",
    "\n",
    "    # Take the detection with highest confidence\n",
    "    best_detection = final_detections[0]\n",
    "\n",
    "    # Return result with integer coordinates\n",
    "    return {\n",
    "        \"tomo_id\": tomo_id,\n",
    "        \"Motor axis 0\": round(best_detection[\"z\"]),\n",
    "        \"Motor axis 1\": round(best_detection[\"y\"]),\n",
    "        \"Motor axis 2\": round(best_detection[\"x\"]),\n",
    "    }\n",
    "\n",
    "\n",
    "def save_submission(submissions, submission_path):\n",
    "    submission_df = pd.DataFrame(submissions)\n",
    "\n",
    "    # Ensure proper column order\n",
    "    submission_df = submission_df[\n",
    "        [\"tomo_id\", \"Motor axis 0\", \"Motor axis 1\", \"Motor axis 2\"]\n",
    "    ]\n",
    "\n",
    "    # Save the submission file\n",
    "    submission_df.to_csv(submission_path, index=False)\n",
    "\n",
    "    print(f\"\\nSubmission complete!\")\n",
    "    print(f\"Submission saved to: {submission_path}\")\n",
    "\n",
    "    # Display first few rows of submission\n",
    "    print(\"\\nSubmission preview:\")\n",
    "    print(submission_df.head())\n",
    "\n",
    "\n",
    "def run_prediction_on_dir(\n",
    "    dir,\n",
    "    threshold=TRESHOLD,\n",
    "    concentration=CONCENTRATION,\n",
    "    submission_path=submission_path,\n",
    "):\n",
    "    tomos = get_tomos(dir)\n",
    "\n",
    "    model = RFDETRBase(\n",
    "        pretrain_weights=\"../working/yolo_model/checkpoint_best_regular.pth\",\n",
    "    )\n",
    "\n",
    "    submissions = []\n",
    "    for tomo_id in tomos:\n",
    "        slice_files = get_slice_files_from_tomo_id(tomo_id, test_dir, concentration)\n",
    "        images = load_images(slice_files, os.path.join(test_dir, tomo_id))\n",
    "\n",
    "        detections_list = []\n",
    "        for image in tqdm.tqdm(images.values()):\n",
    "            detection = model.predict(image, threshold=threshold)\n",
    "            detections_list.append(detection)\n",
    "\n",
    "        all_detections = chain.from_iterable(\n",
    "            make_center_predictions(detection, filename)\n",
    "            for detection, filename in zip(detections_list, images.keys())\n",
    "        )\n",
    "        final_detections = perform_3d_nms(all_detections, NMS_IOU_THRESHOLD)\n",
    "\n",
    "        submissions.append(format_for_submission(tomo_id, final_detections))\n",
    "\n",
    "    save_submission(submissions, submission_path)\n",
    "\n",
    "\n",
    "run_prediction_on_dir(test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5780c81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.metrics\n",
    "\n",
    "\n",
    "class ParticipantVisibleError(Exception):\n",
    "    # If you want an error message to be shown to participants, you must raise the error as a ParticipantVisibleError\n",
    "    # All other errors will only be shown to the competition host. This helps prevent unintentional leakage of solution data.\n",
    "    pass\n",
    "\n",
    "\n",
    "def distance_metric(\n",
    "    solution: pd.DataFrame,\n",
    "    submission: pd.DataFrame,\n",
    "    thresh_ratio: float,\n",
    "    min_radius: float,\n",
    "):\n",
    "    coordinate_cols = [\"Motor axis 0\", \"Motor axis 1\", \"Motor axis 2\"]\n",
    "    label_tensor = solution[coordinate_cols].values.reshape(\n",
    "        len(solution), -1, len(coordinate_cols)\n",
    "    )\n",
    "    predicted_tensor = submission[coordinate_cols].values.reshape(\n",
    "        len(submission), -1, len(coordinate_cols)\n",
    "    )\n",
    "    # Find the minimum euclidean distances between the true and predicted points\n",
    "    solution[\"distance\"] = np.linalg.norm(label_tensor - predicted_tensor, axis=2).min(\n",
    "        axis=1\n",
    "    )\n",
    "    # Convert thresholds from angstroms to voxels\n",
    "    solution[\"thresholds\"] = solution[\"Voxel spacing\"].apply(\n",
    "        lambda x: (min_radius * thresh_ratio) / x\n",
    "    )\n",
    "    solution[\"predictions\"] = submission[\"Has motor\"].values\n",
    "    solution.loc[\n",
    "        (solution[\"distance\"] > solution[\"thresholds\"])\n",
    "        & (solution[\"Has motor\"] == 1)\n",
    "        & (submission[\"Has motor\"] == 1),\n",
    "        \"predictions\",\n",
    "    ] = 0\n",
    "    return solution[\"predictions\"].values\n",
    "\n",
    "\n",
    "def score(\n",
    "    solution: pd.DataFrame, submission: pd.DataFrame, min_radius: float, beta: float\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    solution (pd.DataFrame): DataFrame containing ground truth motor positions.\n",
    "    submission (pd.DataFrame): DataFrame containing predicted motor positions.\n",
    "\n",
    "    Returns:\n",
    "    float: FBeta score.\n",
    "\n",
    "    Example\n",
    "    --------\n",
    "    >>> solution = pd.DataFrame({\n",
    "    ...     'tomo_id': [0, 1, 2, 3],\n",
    "    ...     'Motor axis 0': [-1, 250, 100, 200],\n",
    "    ...     'Motor axis 1': [-1, 250, 100, 200],\n",
    "    ...     'Motor axis 2': [-1, 250, 100, 200],\n",
    "    ...     'Voxel spacing': [10, 10, 10, 10],\n",
    "    ...     'Has motor': [0, 1, 1, 1]\n",
    "    ... })\n",
    "    >>> submission = pd.DataFrame({\n",
    "    ...     'tomo_id': [0, 1, 2, 3],\n",
    "    ...     'Motor axis 0': [100, 251, 600, -1],\n",
    "    ...     'Motor axis 1': [100, 251, 600, -1],\n",
    "    ...     'Motor axis 2': [100, 251, 600, -1]\n",
    "    ... })\n",
    "    >>> score(solution, submission, 1000, 2)\n",
    "    0.3571428571428571\n",
    "    \"\"\"\n",
    "\n",
    "    solution = solution.sort_values(\"tomo_id\").reset_index(drop=True)\n",
    "    submission = submission.sort_values(\"tomo_id\").reset_index(drop=True)\n",
    "\n",
    "    filename_equiv_array = (\n",
    "        solution[\"tomo_id\"].eq(submission[\"tomo_id\"], fill_value=0).values\n",
    "    )\n",
    "\n",
    "    if np.sum(filename_equiv_array) != len(solution[\"tomo_id\"]):\n",
    "        raise ValueError(\n",
    "            \"Submitted tomo_id values do not match the sample_submission file\"\n",
    "        )\n",
    "\n",
    "    submission[\"Has motor\"] = 1\n",
    "    # If any columns are missing an axis, it's marked with no motor\n",
    "    select = (submission[[\"Motor axis 0\", \"Motor axis 1\", \"Motor axis 2\"]] == -1).any(\n",
    "        axis=\"columns\"\n",
    "    )\n",
    "    submission.loc[select, \"Has motor\"] = 0\n",
    "\n",
    "    cols = [\"Has motor\", \"Motor axis 0\", \"Motor axis 1\", \"Motor axis 2\"]\n",
    "    assert all(col in submission.columns for col in cols)\n",
    "\n",
    "    # Calculate a label of 0 or 1 using the 'has motor', and 'motor axis' values\n",
    "    predictions = distance_metric(\n",
    "        solution,\n",
    "        submission,\n",
    "        thresh_ratio=1.0,\n",
    "        min_radius=min_radius,\n",
    "    )\n",
    "\n",
    "    return sklearn.metrics.fbeta_score(\n",
    "        solution[\"Has motor\"].values, predictions, beta=beta\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d774c561",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "import cv2\n",
    "from tqdm.notebook import tqdm\n",
    "import threading\n",
    "import time\n",
    "from contextlib import nullcontext\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Define paths\n",
    "data_path = \"../input\"\n",
    "test_dir = os.path.join(data_path, \"test\")\n",
    "submission_path = \"../working/submission.csv\"\n",
    "\n",
    "# Model path - adjust if your best model is saved in a different location\n",
    "model_path = \"/kaggle/input/train-yolo/yolo_weights/motor_detector/weights/best.pt\"\n",
    "\n",
    "# Detection parameters\n",
    "CONFIDENCE_THRESHOLD = 0.45  # Lower threshold to catch more potential motors\n",
    "MAX_DETECTIONS_PER_TOMO = 3  # Keep track of top N detections per tomogram\n",
    "NMS_IOU_THRESHOLD = 0.2  # Non-maximum suppression threshold for 3D clustering\n",
    "CONCENTRATION = 1  # ONLY PROCESS 1/20 slices for fast submission\n",
    "\n",
    "\n",
    "# GPU profiling context manager\n",
    "class GPUProfiler:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.start_time = None\n",
    "\n",
    "    def __enter__(self):\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()\n",
    "        self.start_time = time.time()\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, *args):\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()\n",
    "        elapsed = time.time() - self.start_time\n",
    "        print(f\"[PROFILE] {self.name}: {elapsed:.3f}s\")\n",
    "\n",
    "\n",
    "# Check GPU availability and set up optimizations\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 8  # Default batch size, will be adjusted dynamically if GPU available\n",
    "\n",
    "if device.startswith(\"cuda\"):\n",
    "    # Set CUDA optimization flags\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cudnn.deterministic = False\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True  # Allow TF32 on Ampere GPUs\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "    # Print GPU info\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1e9  # Convert to GB\n",
    "    print(f\"Using GPU: {gpu_name} with {gpu_mem:.2f} GB memory\")\n",
    "\n",
    "    # Get available GPU memory and set batch size accordingly\n",
    "    free_mem = gpu_mem - torch.cuda.memory_allocated(0) / 1e9\n",
    "    BATCH_SIZE = max(8, min(32, int(free_mem * 4)))  # 4 images per GB as rough estimate\n",
    "    print(\n",
    "        f\"Dynamic batch size set to {BATCH_SIZE} based on {free_mem:.2f}GB free memory\"\n",
    "    )\n",
    "else:\n",
    "    print(\"GPU not available, using CPU\")\n",
    "    BATCH_SIZE = 4  # Reduce batch size for CPU\n",
    "\n",
    "\n",
    "def normalize_slice(slice_data):\n",
    "    \"\"\"\n",
    "    Normalize slice data using 2nd and 98th percentiles for better contrast\n",
    "    \"\"\"\n",
    "    p2 = np.percentile(slice_data, 2)\n",
    "    p98 = np.percentile(slice_data, 98)\n",
    "    clipped_data = np.clip(slice_data, p2, p98)\n",
    "    normalized = 255 * (clipped_data - p2) / (p98 - p2)\n",
    "    return np.uint8(normalized)\n",
    "\n",
    "\n",
    "def preload_image_batch(file_paths):\n",
    "    \"\"\"Preload a batch of images to CPU memory\"\"\"\n",
    "    images = []\n",
    "    for path in file_paths:\n",
    "        img = cv2.imread(path)\n",
    "        if img is None:\n",
    "            # Try with PIL as fallback\n",
    "            img = np.array(Image.open(path))\n",
    "        images.append(img)\n",
    "    return images\n",
    "\n",
    "\n",
    "def process_tomogram(tomo_id, model, index=0, total=1):\n",
    "    \"\"\"\n",
    "    Process a single tomogram and return the most confident motor detection\n",
    "    \"\"\"\n",
    "    print(f\"Processing tomogram {tomo_id} ({index}/{total})\")\n",
    "\n",
    "    # Get all slice files for this tomogram\n",
    "    tomo_dir = os.path.join(test_dir, tomo_id)\n",
    "    slice_files = sorted([f for f in os.listdir(tomo_dir) if f.endswith(\".jpg\")])\n",
    "\n",
    "    # Apply CONCENTRATION to reduce the number of slices processed\n",
    "    # This will process approximately CONCENTRATION fraction of all slices\n",
    "    selected_indices = np.linspace(\n",
    "        0, len(slice_files) - 1, int(len(slice_files) * CONCENTRATION)\n",
    "    )\n",
    "    selected_indices = np.round(selected_indices).astype(int)\n",
    "    slice_files = [slice_files[i] for i in selected_indices]\n",
    "\n",
    "    print(\n",
    "        f\"Processing {len(slice_files)} out of {len(os.listdir(tomo_dir))} slices based on CONCENTRATION={CONCENTRATION}\"\n",
    "    )\n",
    "\n",
    "    # Create a list to store all detections\n",
    "    all_detections = []\n",
    "\n",
    "    # Create CUDA streams for parallel processing if using GPU\n",
    "    if device.startswith(\"cuda\"):\n",
    "        streams = [torch.cuda.Stream() for _ in range(min(4, BATCH_SIZE))]\n",
    "    else:\n",
    "        streams = [None]\n",
    "\n",
    "    # Variables for preloading\n",
    "    next_batch_thread = None\n",
    "    next_batch_images = None\n",
    "\n",
    "    # Process slices in batches\n",
    "    for batch_start in range(0, len(slice_files), BATCH_SIZE):\n",
    "        # Wait for previous preload thread if it exists\n",
    "        if next_batch_thread is not None:\n",
    "            next_batch_thread.join()\n",
    "            next_batch_images = None\n",
    "\n",
    "        batch_end = min(batch_start + BATCH_SIZE, len(slice_files))\n",
    "        batch_files = slice_files[batch_start:batch_end]\n",
    "\n",
    "        # Start preloading next batch\n",
    "        next_batch_start = batch_end\n",
    "        next_batch_end = min(next_batch_start + BATCH_SIZE, len(slice_files))\n",
    "        next_batch_files = (\n",
    "            slice_files[next_batch_start:next_batch_end]\n",
    "            if next_batch_start < len(slice_files)\n",
    "            else []\n",
    "        )\n",
    "\n",
    "        if next_batch_files:\n",
    "            next_batch_paths = [os.path.join(tomo_dir, f) for f in next_batch_files]\n",
    "            next_batch_thread = threading.Thread(\n",
    "                target=preload_image_batch, args=(next_batch_paths,)\n",
    "            )\n",
    "            next_batch_thread.start()\n",
    "        else:\n",
    "            next_batch_thread = None\n",
    "\n",
    "        # Split batch across streams for parallel processing\n",
    "        sub_batches = np.array_split(batch_files, len(streams))\n",
    "        sub_batch_results = []\n",
    "\n",
    "        for i, sub_batch in enumerate(sub_batches):\n",
    "            if len(sub_batch) == 0:\n",
    "                continue\n",
    "\n",
    "            stream = streams[i % len(streams)]\n",
    "            with (\n",
    "                torch.cuda.stream(stream)\n",
    "                if stream and device.startswith(\"cuda\")\n",
    "                else nullcontext()\n",
    "            ):\n",
    "                # Process sub-batch\n",
    "                sub_batch_paths = [\n",
    "                    os.path.join(tomo_dir, slice_file) for slice_file in sub_batch\n",
    "                ]\n",
    "                sub_batch_slice_nums = [\n",
    "                    int(slice_file.split(\"_\")[1].split(\".\")[0])\n",
    "                    for slice_file in sub_batch\n",
    "                ]\n",
    "\n",
    "                # Run inference with profiling\n",
    "                with GPUProfiler(f\"Inference batch {i+1}/{len(sub_batches)}\"):\n",
    "                    sub_results = model(sub_batch_paths, verbose=False)\n",
    "\n",
    "                # Process each result in this sub-batch\n",
    "                for j, result in enumerate(sub_results):\n",
    "                    if len(result.boxes) > 0:\n",
    "                        boxes = result.boxes\n",
    "                        for box_idx, confidence in enumerate(boxes.conf):\n",
    "                            if confidence >= CONFIDENCE_THRESHOLD:\n",
    "                                # Get bounding box coordinates\n",
    "                                x1, y1, x2, y2 = boxes.xyxy[box_idx].cpu().numpy()\n",
    "\n",
    "                                # Calculate center coordinates\n",
    "                                x_center = (x1 + x2) / 2\n",
    "                                y_center = (y1 + y2) / 2\n",
    "\n",
    "                                # Store detection with 3D coordinates\n",
    "                                all_detections.append(\n",
    "                                    {\n",
    "                                        \"z\": round(sub_batch_slice_nums[j]),\n",
    "                                        \"y\": round(y_center),\n",
    "                                        \"x\": round(x_center),\n",
    "                                        \"confidence\": float(confidence),\n",
    "                                    }\n",
    "                                )\n",
    "\n",
    "        # Synchronize streams\n",
    "        if device.startswith(\"cuda\"):\n",
    "            torch.cuda.synchronize()\n",
    "\n",
    "    # Clean up thread if still running\n",
    "    if next_batch_thread is not None:\n",
    "        next_batch_thread.join()\n",
    "\n",
    "    # 3D Non-Maximum Suppression to merge nearby detections across slices\n",
    "    final_detections = perform_3d_nms(all_detections, NMS_IOU_THRESHOLD)\n",
    "\n",
    "    # Sort detections by confidence (highest first)\n",
    "    final_detections.sort(key=lambda x: x[\"confidence\"], reverse=True)\n",
    "\n",
    "    # If there are no detections, return NA values\n",
    "    if not final_detections:\n",
    "        return {\n",
    "            \"tomo_id\": tomo_id,\n",
    "            \"Motor axis 0\": -1,\n",
    "            \"Motor axis 1\": -1,\n",
    "            \"Motor axis 2\": -1,\n",
    "        }\n",
    "\n",
    "    # Take the detection with highest confidence\n",
    "    best_detection = final_detections[0]\n",
    "\n",
    "    # Return result with integer coordinates\n",
    "    return {\n",
    "        \"tomo_id\": tomo_id,\n",
    "        \"Motor axis 0\": round(best_detection[\"z\"]),\n",
    "        \"Motor axis 1\": round(best_detection[\"y\"]),\n",
    "        \"Motor axis 2\": round(best_detection[\"x\"]),\n",
    "    }\n",
    "\n",
    "\n",
    "def perform_3d_nms(detections, iou_threshold):\n",
    "    \"\"\"\n",
    "    Perform 3D Non-Maximum Suppression on detections to merge nearby motors\n",
    "    \"\"\"\n",
    "    if not detections:\n",
    "        return []\n",
    "\n",
    "    # Sort by confidence (highest first)\n",
    "    detections = sorted(detections, key=lambda x: x[\"confidence\"], reverse=True)\n",
    "\n",
    "    # List to store final detections after NMS\n",
    "    final_detections = []\n",
    "\n",
    "    # Define 3D distance function\n",
    "    def distance_3d(d1, d2):\n",
    "        return np.sqrt(\n",
    "            (d1[\"z\"] - d2[\"z\"]) ** 2\n",
    "            + (d1[\"y\"] - d2[\"y\"]) ** 2\n",
    "            + (d1[\"x\"] - d2[\"x\"]) ** 2\n",
    "        )\n",
    "\n",
    "    # Maximum distance threshold (based on box size and slice gap)\n",
    "    box_size = 24  # Same as annotation box size\n",
    "    distance_threshold = box_size * iou_threshold\n",
    "\n",
    "    # Process each detection\n",
    "    while detections:\n",
    "        # Take the detection with highest confidence\n",
    "        best_detection = detections.pop(0)\n",
    "        final_detections.append(best_detection)\n",
    "\n",
    "        # Filter out detections that are too close to the best detection\n",
    "        detections = [\n",
    "            d for d in detections if distance_3d(d, best_detection) > distance_threshold\n",
    "        ]\n",
    "\n",
    "    return final_detections\n",
    "\n",
    "\n",
    "def generate_submission():\n",
    "    \"\"\"\n",
    "    Main function to generate the submission file\n",
    "    \"\"\"\n",
    "    # Get list of test tomograms\n",
    "    test_tomos = sorted(\n",
    "        [d for d in os.listdir(test_dir) if os.path.isdir(os.path.join(test_dir, d))]\n",
    "    )\n",
    "    total_tomos = len(test_tomos)\n",
    "\n",
    "    print(f\"Found {total_tomos} tomograms in test directory\")\n",
    "\n",
    "    # Debug image loading for the first tomogram\n",
    "    if test_tomos:\n",
    "        debug_image_loading(test_tomos[0])\n",
    "\n",
    "    # Clear GPU cache before starting\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # Initialize model once outside the processing loop\n",
    "    print(f\"Loading YOLO model from {model_path}\")\n",
    "    model = YOLO(model_path)\n",
    "    model.to(device)\n",
    "\n",
    "    # Additional optimizations for inference\n",
    "    if device.startswith(\"cuda\"):\n",
    "        # Fuse conv and bn layers for faster inference\n",
    "        model.fuse()\n",
    "\n",
    "        # Enable model half precision (FP16) if on compatible GPU\n",
    "        if torch.cuda.get_device_capability(0)[0] >= 7:  # Volta or newer\n",
    "            model.model.half()\n",
    "            print(\"Using half precision (FP16) for inference\")\n",
    "\n",
    "    # Process tomograms with parallelization\n",
    "    results = []\n",
    "    motors_found = 0\n",
    "\n",
    "    # Using ThreadPoolExecutor with max_workers=1 since each worker uses the GPU already\n",
    "    # and we're parallelizing within each tomogram processing\n",
    "    with ThreadPoolExecutor(max_workers=1) as executor:\n",
    "        future_to_tomo = {}\n",
    "\n",
    "        # Submit all tomograms for processing\n",
    "        for i, tomo_id in enumerate(test_tomos, 1):\n",
    "            future = executor.submit(process_tomogram, tomo_id, model, i, total_tomos)\n",
    "            future_to_tomo[future] = tomo_id\n",
    "\n",
    "        # Process completed futures as they complete\n",
    "        for future in future_to_tomo:\n",
    "            tomo_id = future_to_tomo[future]\n",
    "            try:\n",
    "                # Clear CUDA cache between tomograms\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "                result = future.result()\n",
    "                results.append(result)\n",
    "\n",
    "                # Update motors found count\n",
    "                has_motor = not pd.isna(result[\"Motor axis 0\"])\n",
    "                if has_motor:\n",
    "                    motors_found += 1\n",
    "                    print(\n",
    "                        f\"Motor found in {tomo_id} at position: \"\n",
    "                        f\"z={result['Motor axis 0']}, y={result['Motor axis 1']}, x={result['Motor axis 2']}\"\n",
    "                    )\n",
    "                else:\n",
    "                    print(f\"No motor detected in {tomo_id}\")\n",
    "\n",
    "                print(\n",
    "                    f\"Current detection rate: {motors_found}/{len(results)} ({motors_found/len(results)*100:.1f}%)\"\n",
    "                )\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {tomo_id}: {e}\")\n",
    "                # Create a default entry for failed tomograms\n",
    "                results.append(\n",
    "                    {\n",
    "                        \"tomo_id\": tomo_id,\n",
    "                        \"Motor axis 0\": -1,\n",
    "                        \"Motor axis 1\": -1,\n",
    "                        \"Motor axis 2\": -1,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    # Create submission dataframe\n",
    "    submission_df = pd.DataFrame(results)\n",
    "\n",
    "    # Ensure proper column order\n",
    "    submission_df = submission_df[\n",
    "        [\"tomo_id\", \"Motor axis 0\", \"Motor axis 1\", \"Motor axis 2\"]\n",
    "    ]\n",
    "\n",
    "    # Save the submission file\n",
    "    submission_df.to_csv(submission_path, index=False)\n",
    "\n",
    "    print(f\"\\nSubmission complete!\")\n",
    "    print(\n",
    "        f\"Motors detected: {motors_found}/{total_tomos} ({motors_found/total_tomos*100:.1f}%)\"\n",
    "    )\n",
    "    print(f\"Submission saved to: {submission_path}\")\n",
    "\n",
    "    # Display first few rows of submission\n",
    "    print(\"\\nSubmission preview:\")\n",
    "    print(submission_df.head())\n",
    "\n",
    "    return submission_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "byu-locating-bacterial-flagellar-motors-20-zbi5X9q2-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
